# Example: linux_playbook.yml
## Build a web server
Inside the playbooks folder lets start by creating a simple playbook that
installs an http server (Apache)
```yaml
# linux_playbook.yml
---
- name: Update web servers
  hosts: webservers
  become: true

  tasks:
    - name: Ensure apache is at the present version
      ansible.builtin.dnf:
        name: httpd
        state: present

    - name: Write the apache config file
      ansible.builtin.template:
        src: httpd.conf.j2
        dest: /etc/httpd.conf
        mode: "0644"
```

What this playbook does is:
- targets a group of hosts called `webservers`
- tasks:
  - ensure that the `httpd` package is installed
  - copy a template file to `/etc/httpd.conf`

The playbook includes a task that depends on a template file, so we need to create that file as well, in the same folder as the playbook create a folder called `templates` and inside the folder crate the `httpd.conf.j2` with the following content:

```apache
ServerRoot "/etc/httpd"
Listen 80

User apache
Group apache

ServerAdmin you@example.com
ServerName localhost

DocumentRoot "/var/www/html"
<Directory "/var/www/html">
    Options Indexes FollowSymLinks
    AllowOverride None
    Require all granted
</Directory>

ErrorLog "/var/log/httpd/error_log"
CustomLog "/var/log/httpd/access_log" combined
```

This configuration will let us start a basic web server that will serve files from `/var/www/html`

## Molecule
So what is Molecule?
> Molecule leverages standard Ansible features including inventory, playbooks, and collections to provide flexible testing workflows. Test scenarios can target any system or service reachable from Ansible, from containers and virtual machines to cloud infrastructure, hyperscaler services, APIs, databases, and network devices. Molecule can also validate inventory configurations and dynamic inventory sources.
-- Molecule documentation

Then based on what the documentation says, we are going to setup our molecule
testing environment to target a set of containers that will mimic our real infrastructure.

### Bootstrapping our first scenario
The concept of a scenario is to have a set of resources that will validate our playbook in different contexts, by having in mind our actual infrastructure or environment constraints. The most simplest way from my perspective is that for testing purposes to bring up a set of Linux containers, those are easy to manage, cheap and fast.

Stop talking and show me the code!

```bash
# change directory to the extensions folder
cd extensions
# and initialize a molecule scenario named linux
molecule init scenario linux # name the scenario to something that makes sense to the playbook you are testing
```

This will create a `molecule/linux` directory with an skeleton structure for testing out the playbook.

### molecule.yml
This the entry point for the scenario, here we are going to define all the
parameters for the scenario, like what driver to use, platforms, provisioner,
and many other options that will drive the scenario behavior.

I found out that if so the file generated by the bootstrap command works, is
very simple and some real life scenarios can't be executed properly with this
setup, so I ended up with the following `molecule.yml` file.

```yml
---
dependency:
  name: galaxy
  options:
    requirements-file: requirements.yml

provisioner:
  name: ansible
  config_options:
    defaults:
      collections_path: ${ANSIBLE_COLLECTIONS_PATH}
      # verbosity: 3

platforms:
  - name: web_server
    image: quay.io/centos/centos:stream9
    command: sleep 1d
    privileged: true
    vars:
      http_port: 80
      server_name: test-web
      required_packages:
        - python3
        - systemd
  - name: db_server
    image: quay.io/centos/centos:stream9
    command: sleep 1d
    privileged: true
    vars:
      db_name: testdb
      db_user: testuser
      required_packages:
        - python3
        - systemd
scenario:
  test_sequence:
    - dependency
    - create
    - prepare
    - converge
    - idempotency
    - verify
    - cleanup
    - destroy
```

- dependency: This object instructs molecule how the dependencies are going to
be managed and where those are defined in this case the options in there can be
read in plain as follows, galaxy dependencies are defined in `requirements.yml` file
- provisioner: This part is very interesting it is rendered internally to an
`ansible.cfg` which is read during the scenario execution, as you can see in the
`config_options` object I'm setting the collections_path parameter to a custom
location, this for test isolation purposes. Or the verbosity level (line commented), for debugging purposes, the block also can be used to inject environment variables via the `env` object as child of the `provisioner` object, there are a lot of options that can be included in here but let's keep it simple for now.
- platforms: In here we define our target hosts, the object here is basic it is
  made of a list of instances that at least have a name, of course by having
just a name nothing will be created because of lack of instructions, so this
makes the image parameter some how required. In here you will find a vars block
in which we are providing some variables that will be used later for some
magical stuffs (stay tuned)
- scenario: This object will instruct to `molecule` the order in which the
scenario operations will be executed. By reading the list of elements in the
object you will get the idea, first the dependencies setup by installing
modules, collections, etc; then go and creates stuffs and so on...

### Providing the dependencies
For this example we are going to use the `containers.podman` collection to
handle containers creation in a dynamic way.

`extensions/molecule/linux/requirements.yml`
```yml
---
collections:
  - name: containers.podman
    version: ">=1.10.0"
```

At this pint we should be able to run the dependency step of the scenario, so
let's try it out.

```bash
# cd to the root folder of the project
# setting the collections path to the current working directory
export ANSIBLE_COLLECTIONS_PATHS=$PWD/collections
# go inside the extensions folder
cd extensions
# run the dependency step of the linux scenario
molecule dependency -s linux 
```

After the command finishes you should see that the `containers.podman`
collection installed in the collections folder.

```bash
ansible-galaxy collection list
  Collection        Version
  ----------------- -------
  ansible.netcommon 8.1.0
  ansible.utils     6.0.0
  arista.eos        12.0.0
  containers.podman 1.18.0 # this the one we just installed
```

### Instructing `molecule` how to create the platforms
Something that I found useful to keep the isolation principle we are trying to
follow is that we can customize the way that the platforms are created providing
it some more close-to-reality parameters, like the network configuration, group
membership, custom variables, etc. Let's my approach to achieve that.

1. Creating a custom network for the containers
`extensions/molecule/linux/create.yml`
```yml
---
- name: Create
  hosts: localhost
  connection: local
  gather_facts: false
  tasks:
    - name: Create container network
      containers.podman.podman_network:
        name: molecule-linux-test
        state: present
```
Let's start by indicating that the crate playbook will run in the localhost, our
machine will work as the control node, and we will use the `containers.podman.podman_network` to create an isolated network for our containers, in that way we can guarantee that there will be no interference with other containers that might be running in our machine.

2. Creating the containers
`extensions/molecule/linux/create.yml`
```yml
    - name: Create test containers
      containers.podman.podman_container:
        name: "{{ item.name }}"
        image: "{{ item.image }}"
        command: "{{ item.command }}"
        privileged: "{{ item.privileged }}"
        state: started
        network: molecule-linux-test
        systemd: true
        log_driver: json-file
      loop: "{{ molecule_yml.platforms }}"
      register: create_container
      loop_control:
        label: "{{ item.name }}"

    - name: Fail if container is not running
      when: >
        item.container.State.ExitCode != 0 or
        not item.container.State.Running
      ansible.builtin.include_tasks:
        file: tasks/create-fail.yml
      loop: "{{ create_container.results }}"
      loop_control:
        label: "{{ item.container.Name }}"
```

In here we are instructing to create the containers defined in the
`molecule.yml` by interating over the `molecule_yml.platforms` list, and reading
the parameters defined there, like the image to use, the command to run, etc.
Then after the creation of the containers we are checking that the containers are actually running, if not we will include a task that will fail the playbook execution and print some useful information. Check the `tasks/create-fail.yml` file for more details on what is being checked and how.

3. Creating the inventory file
`extensions/molecule/linux/create.yml`
```yml
    - name: Create testing inventory file
      ansible.builtin.copy:
        content: "{{ lookup('ansible.builtin.template', 'templates/partial_inventory.yaml.j2') }}"
        dest: "{{ molecule_ephemeral_directory }}/inventory/molecule_inventory.yml"
        mode: "0600"

    - name: Force inventory refresh
      ansible.builtin.meta: refresh_inventory

```
Here one of funniest things that I found to work on, building a dynamic scenario
by using Ansible built-in functions, like the `template` within a `lookup` to
read, build and copy a template file to the ephemeral directory that `molecule`
uses as testing inventory. The template file is in the `templates` folder, it
basically templates an inventory file in yaml format, groups the containers by
the group value in the `vars` block of the `molecule.yml` file, and also appends
the `vars` defined for each host in the same way.

Then we force an inventory refresh so the new file is read.
